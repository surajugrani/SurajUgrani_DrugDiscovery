{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dying-culture",
      "metadata": {
        "id": "dying-culture"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import cm\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qualified-battle",
      "metadata": {
        "id": "qualified-battle"
      },
      "outputs": [],
      "source": [
        "def res_analysis(opt_reg, trn_X, trn_y, tst_X, tst_y):\n",
        "    \"\"\"\n",
        "    Perform regression analysis and return evaluation metrics.\n",
        "\n",
        "    Parameters:\n",
        "    - opt_reg: Optimized regression model\n",
        "    - trn_X: Training data features\n",
        "    - trn_y: Training data labels\n",
        "    - tst_X: Test data features\n",
        "    - tst_y: Test data labels\n",
        "\n",
        "    Returns:\n",
        "    - List of evaluation metrics: [R^2, Pearson correlation, RMSE, Spearman correlation]\n",
        "    \"\"\"\n",
        "    otst_pred = pd.Series(opt_reg.predict(tst_X), dtype=float)\n",
        "    otrn_pred = pd.Series(opt_reg.predict(trn_X), dtype=float)\n",
        "    r2 = r2_score(tst_y, otst_pred)\n",
        "    pear = scipy.stats.pearsonr(tst_y, otst_pred)\n",
        "    rmse = (mean_squared_error(tst_y, otst_pred)) ** 0.5\n",
        "    spmn = scipy.stats.spearmanr(tst_y, otst_pred)\n",
        "    return [r2, pear[0], rmse, spmn[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "played-bulgaria",
      "metadata": {
        "id": "played-bulgaria"
      },
      "outputs": [],
      "source": [
        "def find_best_estimator(estimator, parameters, trn_X, trn_y):\n",
        "    \"\"\"\n",
        "    Find the best estimator using GridSearchCV and return it.\n",
        "\n",
        "    Parameters:\n",
        "    - estimator: Regression estimator\n",
        "    - parameters: Parameter grid for GridSearchCV\n",
        "    - trn_X: Training data features\n",
        "    - trn_y: Training data labels\n",
        "\n",
        "    Returns:\n",
        "    - Best estimator from GridSearchCV\n",
        "    \"\"\"\n",
        "    grid_search = GridSearchCV(estimator, parameters, scoring='neg_root_mean_squared_error', cv=8)\n",
        "    model = grid_search.fit(trn_X, trn_y)\n",
        "    print('Training set R-pearson =', scipy.stats.pearsonr(trn_y, model.predict(trn_X)))\n",
        "    return model.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "given-charles",
      "metadata": {
        "id": "given-charles"
      },
      "outputs": [],
      "source": [
        "def eliminate_high_corr(X_df, corr_thresh):\n",
        "    \"\"\"\n",
        "    Eliminate columns with high correlation and return the reduced DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - X_df: Input DataFrame of features\n",
        "    - corr_thresh: Threshold for high correlation\n",
        "    - y: Target variable\n",
        "\n",
        "    Returns:\n",
        "    - Reduced DataFrame and list of eliminated columns\n",
        "    \"\"\"\n",
        "    X_corr = X_df.corr()\n",
        "    high_corr = []\n",
        "    for i in X_df.columns:\n",
        "        for j in X_df.columns:\n",
        "            if i != j and abs(X_corr[i][j]) > corr_thresh:\n",
        "                if (X_df[i].std() <= X_df[j].std()) and i not in high_corr:\n",
        "                    high_corr.append(i)\n",
        "                elif (X_df[i].std() > X_df[j].std()) and j not in high_corr:\n",
        "                    high_corr.append(j)\n",
        "    print(\"Columns to drop:\", len(high_corr), \"\\n\", high_corr)\n",
        "    print(\"Reduced number of columns:\", X_df.shape[1] - len(high_corr))\n",
        "    X_reduced = X_df.drop(high_corr, axis=1)\n",
        "    return [X_reduced, high_corr]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fourth-trader",
      "metadata": {
        "id": "fourth-trader"
      },
      "outputs": [],
      "source": [
        "def fill_pred_dict(dic, spl, nf, y_test, y_pred):\n",
        "    \"\"\"\n",
        "    Fill a prediction dictionary with test and predicted values.\n",
        "\n",
        "    Parameters:\n",
        "    - dic: Prediction dictionary\n",
        "    - spl: Split number\n",
        "    - nf: Number of features\n",
        "    - y_test: True labels from the test set\n",
        "    - y_pred: Predicted labels\n",
        "\n",
        "    Returns:\n",
        "    - Updated prediction dictionary\n",
        "    \"\"\"\n",
        "    dic['test_split-' + str(spl) + '_features-' + str(nf)] = np.array(y_test)\n",
        "    dic['pred_split-' + str(spl) + '_features-' + str(nf)] = np.array(y_pred)\n",
        "    return dic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "native-genetics",
      "metadata": {
        "id": "native-genetics"
      },
      "outputs": [],
      "source": [
        "def plot_and_save(estimator, split, num_features, y_test, y_pred):\n",
        "    \"\"\"\n",
        "    Create a scatter plot of y_test vs y_pred, add a reference line, set plot properties,\n",
        "    and save the plot as an image file.\n",
        "\n",
        "    Parameters:\n",
        "        estimator (str): Name or description of the estimator/model used.\n",
        "        split (int): Split or fold number.\n",
        "        num_features (int): Number of features used in the model.\n",
        "        y_test (array-like): True target values.\n",
        "        y_pred (array-like): Predicted target values.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.scatter(y_test, y_pred, alpha=0.6, color='blue', label='Actual vs. Predicted')\n",
        "    plt.plot([-9, -2], [-9, -2], linestyle='--', color='black', label='Perfect Match')\n",
        "\n",
        "    title = f'Split {split} | {num_features} Features | Estimator: {estimator}'\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.xlabel('Actual y_test (kcal/mol)', fontsize=14)\n",
        "    plt.ylabel('Predicted y_pred (kcal/mol)', fontsize=14)\n",
        "    plt.legend(loc='upper left', fontsize=12)\n",
        "    plt.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    save_filename = f'Split{split}_Features{num_features}_{estimator}.png'\n",
        "    plt.savefig(save_filename)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "# plot_and_save('Random Forest', 1, 10, y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "former-upper",
      "metadata": {
        "id": "former-upper"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "# Constants\n",
        "VARIANCE_THRESHOLD = 0.01\n",
        "CORRELATION_THRESHOLD = 0.7\n",
        "\n",
        "def preprocess_data(df):\n",
        "    # Remove columns with low variance\n",
        "    to_drop = df.columns[-1:]\n",
        "    X = df.drop(to_drop, axis=1)\n",
        "\n",
        "    # Apply variance threshold and correlation threshold\n",
        "    vt = VarianceThreshold(VARIANCE_THRESHOLD)\n",
        "    X0 = pd.DataFrame(vt.fit_transform(X), index=X.index, columns=X.columns[vt.get_support()])\n",
        "    X1 = eliminate_high_corr(X0, CORRELATION_THRESHOLD)[0]\n",
        "    return X1\n",
        "\n",
        "# Load your data into DF0\n",
        "DF0 = pd.read_excel(\"DF_1174X50_04-17-2023.xlsx\", index_col=0)\n",
        "DF0 = DF0.drop(columns=['Name'])\n",
        "\n",
        "# Preprocess the data\n",
        "X2 = preprocess_data(DF0)\n",
        "y = DF0.iloc[:,-1]\n",
        "\n",
        "# Define a list of numbers of features to consider\n",
        "num_features = [2, 4, 6, 8, 10, 12, X2.shape[1]]\n",
        "\n",
        "# Create empty DataFrames for results\n",
        "result_columns = ['RF', 'GBR', 'SVR', 'LR']\n",
        "result_df_names = [f'{model}{nf}' for nf in num_features for model in result_columns]\n",
        "\n",
        "spl_is = [f'split_{i}' for i in range(10)]\n",
        "feaimpdf = pd.DataFrame(0, index=spl_is, columns=X2.columns)\n",
        "r2df = pd.DataFrame(index=spl_is, columns=result_df_names)\n",
        "peardf = pd.DataFrame(index=spl_is, columns=result_df_names)\n",
        "rmsedf = pd.DataFrame(index=spl_is, columns=result_df_names)\n",
        "spmndf = pd.DataFrame(index=spl_is, columns=result_df_names)\n",
        "\n",
        "lin_dict_pred = {}\n",
        "gbr_dict_pred = {}\n",
        "svr_dict_pred = {}\n",
        "rfr_dict_pred = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "split-damage",
      "metadata": {
        "id": "split-damage"
      },
      "outputs": [],
      "source": [
        "ntfs = [100,150,200,250]\n",
        "maxfeafs =['sqrt','log2']\n",
        "minsmplffs = [1,3,5]\n",
        "parfs = {'max_features':maxfeafs, 'n_estimators':ntfs, 'min_samples_leaf':minsmplffs}\n",
        "\n",
        "ne = [100,200,300,400,500]\n",
        "minsmpleaf = [1,3,5]\n",
        "mxf =  ['sqrt','log2']\n",
        "rfpar = {'n_estimators':ne, 'max_features':mxf, 'min_samples_leaf':minsmpleaf}\n",
        "\n",
        "gbrne = [100,150,200,250]\n",
        "gbrpar = {'max_features':mxf, 'n_estimators':gbrne, 'min_samples_leaf':minsmpleaf}\n",
        "\n",
        "c = np.logspace(-1,1,15)\n",
        "gam = ['scale']\n",
        "ker = ['rbf']\n",
        "svrpar = {'C':c, 'gamma':gam, 'kernel':ker}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "living-bronze",
      "metadata": {
        "id": "living-bronze"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import scipy.stats\n",
        "\n",
        "# Constants\n",
        "NUM_SPLITS = 2\n",
        "RANDOM_SEED = 17061991\n",
        "\n",
        "def main():\n",
        "\n",
        "    num_features_to_try = [2, 4, 6, 8, 10, 12, X2.shape[1]]\n",
        "    result_columns = ['RF', 'GBR', 'SVR', 'LR']\n",
        "\n",
        "    for spl in range(NUM_SPLITS):\n",
        "        print('\\nCurrently running Split', spl)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size=0.2, random_state=RANDOM_SEED + (43210 * spl))\n",
        "\n",
        "        gbrfs = GradientBoostingRegressor(random_state=RANDOM_SEED + (54321 * spl))\n",
        "        gbrfs_best = find_best_estimator(gbrfs, parfs, X_train, y_train)\n",
        "        gbrfs_features = X2.columns[gbrfs_best.feature_importances_.argsort()[::-1][:num_features_to_try[-1]]]\n",
        "\n",
        "        for nf in num_features_to_try:\n",
        "            print(\"\\nNumber Of Features:\", nf)\n",
        "            top_features = gbrfs_features[:nf]\n",
        "            X_trn, X_tst = X_train[top_features], X_test[top_features]\n",
        "\n",
        "            linr = LinearRegression()\n",
        "            linr_fit = linr.fit(X_trn, y_train)\n",
        "            lr_r_squared, lr_pearson_r, lr_rmse, lr_spmn = res_analysis(linr_fit, X_trn, y_train, X_tst, y_test)\n",
        "\n",
        "            svr = SVR()\n",
        "            svr_best = find_best_estimator(svr, svrpar, X_trn, y_train)\n",
        "            svr_r_squared, svr_pearson_r, svr_rmse, svr_spmn = res_analysis(svr_best, X_trn, y_train, X_tst, y_test)\n",
        "\n",
        "            gbr = GradientBoostingRegressor(random_state=RANDOM_SEED + (65432 * spl))\n",
        "            gbr_best = find_best_estimator(gbr, gbrpar, X_trn, y_train)\n",
        "            gbr_r_squared, gbr_pearson_r, gbr_rmse, gbr_spmn = res_analysis(gbr_best, X_trn, y_train, X_tst, y_test)\n",
        "\n",
        "            rfr = RandomForestRegressor(random_state=RANDOM_SEED + (76543 * spl))\n",
        "            rfr_best = find_best_estimator(rfr, rfpar, X_trn, y_train)\n",
        "            rfr_r_squared, rfr_pearson_r, rfr_rmse, rfr_spmn = res_analysis(rfr_best, X_trn, y_train, X_tst, y_test)\n",
        "\n",
        "            # Store the results in DataFrames\n",
        "\n",
        "            # Plot and save the results if needed\n",
        "            # plot_and_save('LR', spl, nf, y_test, linr.predict(X_tst))\n",
        "            # plot_and_save('SVR', spl, nf, y_test, svr_best.predict(X_tst))\n",
        "            # plot_and_save('GBR', spl, nf, y_test, gbr_best.predict(X_tst))\n",
        "            # plot_and_save('RFR', spl, nf, y_test, rfr_best.predict(X_tst))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "proof-dubai",
      "metadata": {
        "id": "proof-dubai"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}