{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dying-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "qualified-battle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_analysis(opt_reg, trn_X, trn_y, tst_X, tst_y):\n",
    "    \"\"\"\n",
    "    Perform regression analysis and return evaluation metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    - opt_reg: Optimized regression model\n",
    "    - trn_X: Training data features\n",
    "    - trn_y: Training data labels\n",
    "    - tst_X: Test data features\n",
    "    - tst_y: Test data labels\n",
    "    \n",
    "    Returns:\n",
    "    - List of evaluation metrics: [R^2, Pearson correlation, RMSE, Spearman correlation]\n",
    "    \"\"\"\n",
    "    otst_pred = pd.Series(opt_reg.predict(tst_X), dtype=float)\n",
    "    otrn_pred = pd.Series(opt_reg.predict(trn_X), dtype=float)\n",
    "    r2 = r2_score(tst_y, otst_pred)\n",
    "    pear = scipy.stats.pearsonr(tst_y, otst_pred)\n",
    "    rmse = (mean_squared_error(tst_y, otst_pred)) ** 0.5\n",
    "    spmn = scipy.stats.spearmanr(tst_y, otst_pred)\n",
    "    return [r2, pear[0], rmse, spmn[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "played-bulgaria",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_estimator(estimator, parameters, trn_X, trn_y):\n",
    "    \"\"\"\n",
    "    Find the best estimator using GridSearchCV and return it.\n",
    "    \n",
    "    Parameters:\n",
    "    - estimator: Regression estimator\n",
    "    - parameters: Parameter grid for GridSearchCV\n",
    "    - trn_X: Training data features\n",
    "    - trn_y: Training data labels\n",
    "    \n",
    "    Returns:\n",
    "    - Best estimator from GridSearchCV\n",
    "    \"\"\"\n",
    "    grid_search = GridSearchCV(estimator, parameters, scoring='neg_root_mean_squared_error', cv=8)\n",
    "    model = grid_search.fit(trn_X, trn_y)\n",
    "    print('Training set R-pearson =', scipy.stats.pearsonr(trn_y, model.predict(trn_X)))\n",
    "    return model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "given-charles",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_high_corr(X_df, corr_thresh):\n",
    "    \"\"\"\n",
    "    Eliminate columns with high correlation and return the reduced DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_df: Input DataFrame of features\n",
    "    - corr_thresh: Threshold for high correlation\n",
    "    - y: Target variable\n",
    "    \n",
    "    Returns:\n",
    "    - Reduced DataFrame and list of eliminated columns\n",
    "    \"\"\"\n",
    "    X_corr = X_df.corr()\n",
    "    high_corr = []\n",
    "    for i in X_df.columns:\n",
    "        for j in X_df.columns:\n",
    "            if i != j and abs(X_corr[i][j]) > corr_thresh:\n",
    "                if (X_df[i].std() <= X_df[j].std()) and i not in high_corr:\n",
    "                    high_corr.append(i)\n",
    "                elif (X_df[i].std() > X_df[j].std()) and j not in high_corr:\n",
    "                    high_corr.append(j)\n",
    "    print(\"Columns to drop:\", len(high_corr), \"\\n\", high_corr)\n",
    "    print(\"Reduced number of columns:\", X_df.shape[1] - len(high_corr))\n",
    "    X_reduced = X_df.drop(high_corr, axis=1)\n",
    "    return [X_reduced, high_corr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fourth-trader",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_pred_dict(dic, spl, nf, y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Fill a prediction dictionary with test and predicted values.\n",
    "    \n",
    "    Parameters:\n",
    "    - dic: Prediction dictionary\n",
    "    - spl: Split number\n",
    "    - nf: Number of features\n",
    "    - y_test: True labels from the test set\n",
    "    - y_pred: Predicted labels\n",
    "    \n",
    "    Returns:\n",
    "    - Updated prediction dictionary\n",
    "    \"\"\"\n",
    "    dic['test_split-' + str(spl) + '_features-' + str(nf)] = np.array(y_test)\n",
    "    dic['pred_split-' + str(spl) + '_features-' + str(nf)] = np.array(y_pred)\n",
    "    return dic    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "native-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save(estimator, split, num_features, y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Create a scatter plot of y_test vs y_pred, add a reference line, set plot properties,\n",
    "    and save the plot as an image file.\n",
    "\n",
    "    Parameters:\n",
    "        estimator (str): Name or description of the estimator/model used.\n",
    "        split (int): Split or fold number.\n",
    "        num_features (int): Number of features used in the model.\n",
    "        y_test (array-like): True target values.\n",
    "        y_pred (array-like): Predicted target values.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.6, color='blue', label='Actual vs. Predicted')\n",
    "    plt.plot([-9, -2], [-9, -2], linestyle='--', color='black', label='Perfect Match')\n",
    "    \n",
    "    title = f'Split {split} | {num_features} Features | Estimator: {estimator}'\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Actual y_test (kcal/mol)', fontsize=14)\n",
    "    plt.ylabel('Predicted y_pred (kcal/mol)', fontsize=14)\n",
    "    plt.legend(loc='upper left', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    \n",
    "    save_filename = f'Split{split}_Features{num_features}_{estimator}.png'\n",
    "    plt.savefig(save_filename)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# plot_and_save('Random Forest', 1, 10, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "former-upper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to drop: 1 \n",
      " [1]\n",
      "Reduced number of columns: 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Constants\n",
    "VARIANCE_THRESHOLD = 0.01\n",
    "CORRELATION_THRESHOLD = 0.7\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Remove columns with low variance\n",
    "    to_drop = df.columns[-1:]\n",
    "    X = df.drop(to_drop, axis=1)\n",
    "\n",
    "    # Apply variance threshold and correlation threshold\n",
    "    vt = VarianceThreshold(VARIANCE_THRESHOLD)\n",
    "    X0 = pd.DataFrame(vt.fit_transform(X), index=X.index, columns=X.columns[vt.get_support()])\n",
    "    X1 = eliminate_high_corr(X0, CORRELATION_THRESHOLD)[0]\n",
    "    return X1\n",
    "\n",
    "# Load your data into DF0\n",
    "DF0 = pd.read_excel(\"DataFrame.xlsx\", index_col=0)\n",
    "\n",
    "# Preprocess the data\n",
    "X2 = preprocess_data(DF0)\n",
    "y = DF0.iloc[:,-1]\n",
    "\n",
    "# Define a list of numbers of features to consider\n",
    "# num_features = [2, 4, 6, 8, 10, 12, X2.shape[1]]\n",
    "\n",
    "# Create empty DataFrames for results\n",
    "result_columns = ['RF', 'GBR', 'SVR', 'LR']\n",
    "result_df_names = [f'{model}{nf}' for nf in num_features for model in result_columns]\n",
    "\n",
    "spl_is = [f'split_{i}' for i in range(10)]\n",
    "feaimpdf = pd.DataFrame(0, index=spl_is, columns=X2.columns)\n",
    "r2df = pd.DataFrame(index=spl_is, columns=result_df_names)\n",
    "peardf = pd.DataFrame(index=spl_is, columns=result_df_names)\n",
    "rmsedf = pd.DataFrame(index=spl_is, columns=result_df_names)\n",
    "spmndf = pd.DataFrame(index=spl_is, columns=result_df_names)\n",
    "\n",
    "lin_dict_pred = {}\n",
    "gbr_dict_pred = {}\n",
    "svr_dict_pred = {}\n",
    "rfr_dict_pred = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "split-damage",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntfs = [100,150,200,250]\n",
    "maxfeafs =['sqrt','log2','auto']\n",
    "minsmplffs = [1,3,5]\n",
    "parfs = {'max_features':maxfeafs, 'n_estimators':ntfs, 'min_samples_leaf':minsmplffs}\n",
    "\n",
    "ne = [100,200,300,400,500]\n",
    "minsmpleaf = [1,3,5]\n",
    "mxf =  ['sqrt','log2','auto']\n",
    "rfpar = {'n_estimators':ne, 'max_features':mxf, 'min_samples_leaf':minsmpleaf} \n",
    "\n",
    "gbrne = [100,150,200,250]\n",
    "gbrpar = {'max_features':mxf, 'n_estimators':gbrne, 'min_samples_leaf':minsmpleaf}\n",
    "\n",
    "c = np.logspace(-1,1,15)\n",
    "gam = ['scale','auto']\n",
    "ker = ['rbf']  \n",
    "svrpar = {'C':c, 'gamma':gam, 'kernel':ker}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "living-bronze",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Currently running Split 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot have number of splits n_splits=8 greater than the number of samples: n_samples=2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-079c8e68ba57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-50-079c8e68ba57>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mgbr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGradientBoostingRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRANDOM_SEED\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m54321\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mspl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mgbr_best\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_best_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgbr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mgbr_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgbr_best\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnum_features_to_try\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-521ef32998fe>\u001b[0m in \u001b[0;36mfind_best_estimator\u001b[1;34m(estimator, parameters, trn_X, trn_y)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \"\"\"\n\u001b[0;32m     14\u001b[0m     \u001b[0mgrid_search\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'neg_root_mean_squared_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrn_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrn_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training set R-pearson ='\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpearsonr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrn_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrn_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    839\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1286\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1288\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    807\u001b[0m                                    (split_idx, (train, test)) in product(\n\u001b[0;32m    808\u001b[0m                                    \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 809\u001b[1;33m                                    enumerate(cv.split(X, y, groups))))\n\u001b[0m\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    328\u001b[0m                 (\"Cannot have number of splits n_splits={0} greater\"\n\u001b[0;32m    329\u001b[0m                  \" than the number of samples: n_samples={1}.\")\n\u001b[1;32m--> 330\u001b[1;33m                 .format(self.n_splits, n_samples))\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot have number of splits n_splits=8 greater than the number of samples: n_samples=2."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import scipy.stats\n",
    "\n",
    "# Constants\n",
    "NUM_SPLITS = 10\n",
    "RANDOM_SEED = 17061991\n",
    "\n",
    "def main():\n",
    "    # Load your data into DF03\n",
    "    \n",
    "    #X2 = preprocess_data(DF03)\n",
    "    \n",
    "    num_features_to_try = [2, 4, 6, 8, 10, 12, X2.shape[1]]\n",
    "    result_columns = ['RF', 'GBR', 'SVR', 'LR']\n",
    "\n",
    "    for spl in range(NUM_SPLITS):\n",
    "        print('\\nCurrently running Split', spl)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size=0.2, random_state=RANDOM_SEED + (43210 * spl))\n",
    "        \n",
    "        gbrfs = GradientBoostingRegressor(random_state=RANDOM_SEED + (54321 * spl))\n",
    "        gbrfs_best = find_best_estimator(gbrfs, parfs, X_train, y_train)\n",
    "        gbrfs_features = X2.columns[gbrfs_best.feature_importances_.argsort()[::-1][:num_features_to_try[-1]]]\n",
    "        \n",
    "        for nf in num_features_to_try:\n",
    "            print(\"\\nNumber Of Features:\", nf)\n",
    "            top_features = gbr_features[:nf]\n",
    "            X_trn, X_tst = X_train[top_features], X_test[top_features]\n",
    "            \n",
    "            linr = LinearRegression()\n",
    "            linr_fit = linr.fit(X_trn, y_train)\n",
    "            lr_r_squared, lr_pearson_r, lr_rmse, lr_spmn = res_analysis(linr_fit, X_trn, y_train, X_tst, y_test)\n",
    "            \n",
    "            svr = SVR()\n",
    "            svr_best = find_best_estimator(svr, svrpar, X_trn, y_train)\n",
    "            svr_r_squared, svr_pearson_r, svr_rmse, svr_spmn = res_analysis(svr_best, X_trn, y_train, X_tst, y_test)\n",
    "            \n",
    "            gbr = GradientBoostingRegressor(random_state=RANDOM_SEED + (65432 * spl))\n",
    "            gbr_best = find_best_estimator(gbr, gbrpar, X_trn, y_train)\n",
    "            gbr_r_squared, gbr_pearson_r, gbr_rmse, gbr_spmn = res_analysis(gbr_best, X_trn, y_train, X_tst, y_test)\n",
    "            \n",
    "            rfr = RandomForestRegressor(random_state=RANDOM_SEED + (76543 * spl))\n",
    "            rfr_best = find_best_estimator(rfr, rfpar, X_trn, y_train)\n",
    "            rfr_r_squared, rfr_pearson_r, rfr_rmse, rfr_spmn = res_analysis(rfr_best, X_trn, y_train, X_tst, y_test)\n",
    "            \n",
    "            # Store the results in DataFrames\n",
    "            \n",
    "            # Plot and save the results if needed\n",
    "            # plot_and_save('LR', spl, nf, y_test, linr.predict(X_tst))\n",
    "            # plot_and_save('SVR', spl, nf, y_test, svr_best.predict(X_tst))\n",
    "            # plot_and_save('GBR', spl, nf, y_test, gbr_best.predict(X_tst))\n",
    "            # plot_and_save('RFR', spl, nf, y_test, rfr_best.predict(X_tst))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-dubai",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
